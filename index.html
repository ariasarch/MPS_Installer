<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>MPS - Miniscope Processing Software</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <!-- Favicon -->
    <link rel="icon" type="image/png" href="https://raw.githubusercontent.com/ariasarch/MPS_installer/main/neumaierlabdesign.png">
    
    <!-- Open Graph / Link Preview Tags -->
    <meta property="og:title" content="MPS - Miniscope Processing Software" />
    <meta property="og:description" content="Professional calcium imaging analysis pipeline" />
    <meta property="og:image" content="https://raw.githubusercontent.com/ariasarch/MPS_installer/main/neumaierlabdesign.png" />
    <meta property="og:url" content="https://ariasarch.github.io/MPS_installer/" />
    <meta property="og:type" content="website" />
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Inter', 'Helvetica Neue', Arial, sans-serif;
        }
        
        body {
            background-color: #000000;
            color: #ffffff;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        
        .container {
            max-width: 1000px;
            width: 100%;
            padding: 20px;
        }
        
        .header {
            text-align: center;
            padding: 60px 0 40px;
        }
        
        .logo-container {
            margin-bottom: 20px;
        }
        
        .logo-image {
            max-width: 400px;
            width: 100%;
            height: auto;
        }
        
        .tagline {
            font-size: 24px;
            color: #a9a9a9;
            margin-bottom: 20px;
            font-weight: 300;
        }
        
        .version-badge {
            display: inline-block;
            background-color: #2a2a2a;
            color: #cccccc;
            padding: 6px 16px;
            border-radius: 20px;
            font-size: 14px;
            margin-top: 10px;
        }
        
        /* Navigation Tabs */
        .nav-tabs {
            display: flex;
            justify-content: center;
            gap: 10px;
            margin: 40px 0;
            border-bottom: 2px solid #2a2a2a;
            padding-bottom: 0;
        }
        
        .nav-tab {
            padding: 12px 24px;
            background: transparent;
            color: #a9a9a9;
            border: none;
            border-bottom: 2px solid transparent;
            cursor: pointer;
            font-size: 16px;
            font-weight: 500;
            transition: all 0.3s ease;
            margin-bottom: -2px;
        }
        
        .nav-tab:hover {
            color: #ffffff;
            background-color: #1a1a1a;
        }
        
        .nav-tab.active {
            color: #ffffff;
            border-bottom-color: #ffffff;
        }
        
        /* Tab Content */
        .tab-content {
            display: none;
        }
        
        .tab-content.active {
            display: block;
        }
        
        .section {
            margin: 60px 0;
        }
        
        .section-title {
            font-size: 32px;
            margin-bottom: 20px;
            color: #ffffff;
            position: relative;
            display: inline-block;
            font-weight: 600;
        }
        
        .section-title:after {
            content: '';
            position: absolute;
            left: 0;
            bottom: -8px;
            width: 50px;
            height: 3px;
            background: #444;
        }
        
        .section-content {
            font-size: 18px;
            line-height: 1.8;
            color: #cccccc;
            margin-bottom: 20px;
        }
        
        .features {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
            margin: 60px 0;
        }
        
        .feature {
            background-color: #1e1e1e;
            border-radius: 12px;
            padding: 30px;
            transition: all 0.3s ease;
            border: 1px solid #2a2a2a;
        }
        
        .feature:hover {
            transform: translateY(-5px);
            border-color: #444;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
        }
        
        .feature-icon {
            font-size: 36px;
            margin-bottom: 20px;
        }
        
        .feature-title {
            font-size: 22px;
            margin-bottom: 12px;
            color: #ffffff;
            font-weight: 600;
        }
        
        .feature-description {
            font-size: 16px;
            color: #a9a9a9;
            line-height: 1.6;
        }
        
        .cta-section {
            text-align: center;
            margin: 80px 0;
            padding: 60px;
            background-color: #1a1a1a;
            border-radius: 16px;
            border: 1px solid #2a2a2a;
        }
        
        .cta-title {
            font-size: 36px;
            margin-bottom: 16px;
            font-weight: 600;
        }
        
        .cta-subtitle {
            font-size: 20px;
            color: #a9a9a9;
            margin-bottom: 40px;
        }
        
        .download-button {
            display: inline-flex;
            align-items: center;
            gap: 12px;
            background: linear-gradient(90deg, #3a3a3a, #4a4a4a);
            color: white;
            padding: 20px 40px;
            border-radius: 30px;
            font-size: 20px;
            font-weight: 600;
            text-decoration: none;
            transition: all 0.3s ease;
            border: none;
            cursor: pointer;
        }
        
        .download-button:hover {
            background: linear-gradient(90deg, #4a4a4a, #5a5a5a);
            transform: translateY(-2px);
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.5);
        }
        
        .download-info {
            margin-top: 20px;
            font-size: 14px;
            color: #777;
        }
        
        .requirements {
            background-color: #1a1a1a;
            border: 1px solid #2a2a2a;
            padding: 40px;
            border-radius: 12px;
            margin: 40px 0;
        }
        
        .requirements h3 {
            color: #ffffff;
            margin-bottom: 20px;
            font-size: 24px;
        }
        
        .requirements ul {
            list-style: none;
            padding-left: 0;
        }
        
        .requirements li {
            padding: 12px 0;
            border-bottom: 1px solid #2a2a2a;
            color: #cccccc;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .requirements li:last-child {
            border-bottom: none;
        }
        
        .requirements li:before {
            content: "‚úì";
            color: #666;
            font-weight: bold;
            font-size: 18px;
        }
        
        .warning-box {
            background-color: #2a2a2a;
            border: 1px solid #444;
            color: #f0f0f0;
            padding: 20px;
            border-radius: 8px;
            margin: 30px 0;
            font-size: 16px;
            line-height: 1.6;
        }
        
        .warning-box strong {
            color: #ffffff;
        }
        
        footer {
            margin-top: auto;
            width: 100%;
            background-color: #000000;
            padding: 30px 0;
            text-align: center;
            border-top: 1px solid #2a2a2a;
        }
        
        .footer-content {
            max-width: 1000px;
            margin: 0 auto;
            padding: 0 20px;
        }
        
        .footer-links {
            display: flex;
            justify-content: center;
            gap: 30px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }
        
        .footer-link {
            color: #a9a9a9;
            text-decoration: none;
            transition: color 0.2s ease;
            font-size: 16px;
        }
        
        .footer-link:hover {
            color: #ffffff;
        }
        
        .footer-text {
            font-size: 14px;
            color: #666;
            margin-top: 20px;
        }
        
        .code-block {
            background-color: #000000;
            border: 1px solid #2a2a2a;
            border-radius: 8px;
            padding: 20px;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 14px;
            color: #cccccc;
            overflow-x: auto;
            margin: 20px 0;
        }
        
        /* Documentation specific styles */
        .doc-container {
            background-color: #1a1a1a;
            border-radius: 16px;
            padding: 40px;
            margin: 20px 0;
            border: 1px solid #2a2a2a;
        }
        
        .doc-toc {
            background-color: #1a1a1a;
            border: 1px solid #2a2a2a;
            border-radius: 12px;
            padding: 30px;
            margin: 30px 0;
        }
        
        .doc-toc h3 {
            color: #ffffff;
            margin-bottom: 20px;
            font-size: 24px;
        }
        
        .doc-toc ul {
            list-style: none;
            padding-left: 0;
        }
        
        .doc-toc li {
            padding: 8px 0;
        }
        
        .doc-toc a {
            color: #4a9eff;
            text-decoration: none;
            transition: color 0.2s ease;
        }
        
        .doc-toc a:hover {
            color: #6ab7ff;
            text-decoration: underline;
        }
        
        .doc-section {
            margin: 40px 0;
            padding: 30px;
            background-color: #1a1a1a;
            border-radius: 12px;
            border: 1px solid #2a2a2a;
        }
        
        .doc-section h3 {
            color: #ffffff;
            font-size: 28px;
            margin-bottom: 20px;
            font-weight: 600;
        }
        
        .doc-section h4 {
            color: #ffffff;
            font-size: 22px;
            margin: 30px 0 15px;
            font-weight: 600;
        }
        
        .doc-section h5 {
            color: #ffffff;
            font-size: 18px;
            margin: 20px 0 10px;
            font-weight: 600;
        }
        
        .doc-section p {
            color: #cccccc;
            line-height: 1.8;
            margin-bottom: 15px;
        }
        
        .doc-section ul, .doc-section ol {
            color: #cccccc;
            line-height: 1.8;
            margin: 15px 0;
            padding-left: 30px;
        }
        
        .doc-section li {
            margin: 8px 0;
        }
        
        .parameter-box {
            background-color: #0a0a0a;
            border: 1px solid #333;
            border-radius: 8px;
            padding: 20px;
            margin: 15px 0;
        }
        
        .parameter-box strong {
            color: #4a9eff;
        }
        
        .tip-box {
            background-color: #1a2a1a;
            border: 1px solid #2a4a2a;
            border-left: 4px solid #4a9eff;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .tip-box strong {
            color: #4a9eff;
        }
        
        @media (max-width: 768px) {
            .logo-image {
                max-width: 300px;
            }
            
            .tagline {
                font-size: 20px;
            }
            
            .section-title {
                font-size: 28px;
            }
            
            .cta-section {
                padding: 40px 20px;
            }
            
            .download-button {
                padding: 16px 32px;
                font-size: 18px;
            }
            
            .nav-tabs {
                flex-wrap: wrap;
                justify-content: center;
            }
            
            .nav-tab {
                font-size: 14px;
                padding: 10px 16px;
            }
            
            .doc-section {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="header">
            <div class="logo-container">
                <img src="https://raw.githubusercontent.com/ariasarch/MPS_installer/main/neumaierlabdesign.png" alt="Neumaier Lab" class="logo-image">
            </div>
            <p class="tagline">Miniscope Processing Software</p>
            <span class="version-badge">Version 1.0.0</span>
        </header>

        <!-- Navigation Tabs -->
        <div class="nav-tabs">
            <button class="nav-tab active" onclick="showTab('overview')">Overview</button>
            <button class="nav-tab" onclick="showTab('documentation')">Documentation</button>
            <button class="nav-tab" onclick="showTab('installation')">Installation</button>
            <button class="nav-tab" onclick="showTab('resources')">Resources</button>
        </div>

        <!-- Overview Tab -->
        <div id="overview" class="tab-content active">
            <section class="section">
                <h2 class="section-title">üî¨ Professional Calcium Imaging Analysis</h2>
                <p class="section-content">
                    MPS is a comprehensive pipeline for processing miniscope calcium imaging data. From raw videos to publication-ready neural activity traces, MPS guides you through every step with an intuitive interface and powerful algorithms.
                </p>
                <p class="section-content">
                    Built by neuroscientists for neuroscientists, MPS implements the gold-standard CNMF (Constrained Non-negative Matrix Factorization) algorithm, now enhanced with multi-lasso for advanced spatial segmentation and optimized temporal component updates‚Äîall optimized for large-scale datasets.
                </p>
            </section>
            
            <div class="features">
                <div class="feature">
                    <div class="feature-icon">üß†</div>
                    <h3 class="feature-title">Complete Pipeline</h3>
                    <p class="feature-description">From raw miniscope videos to analyzed neural activity. Includes motion correction, CNMF-based segmentation with NNDSVD initialization, and advanced temporal processing.</p>
                </div>
                
                <div class="feature">
                    <div class="feature-icon">‚ö°</div>
                    <h3 class="feature-title">Optimized Performance</h3>
                    <p class="feature-description">Leverages Dask for parallel processing and memory-efficient computation. Process hours of recordings without running out of RAM.</p>
                </div>
                
                <div class="feature">
                    <div class="feature-icon">üìä</div>
                    <h3 class="feature-title">Real-time Visualization</h3>
                    <p class="feature-description">Monitor your analysis progress with live previews and quality metrics. Debug terminal shows detailed processing information.</p>
                </div>
                
                <div class="feature">
                    <div class="feature-icon">üíæ</div>
                    <h3 class="feature-title">Flexible Export</h3>
                    <p class="feature-description">Export to multiple formats including Zarr, NumPy, and JSON. Compatible with downstream analysis in Python, MATLAB, or R.</p>
                </div>
                
                <div class="feature">
                    <div class="feature-icon">üîÑ</div>
                    <h3 class="feature-title">Reproducible Science</h3>
                    <p class="feature-description">Save and share parameter files to ensure consistent results across sessions, datasets, and collaborators.</p>
                </div>
                
                <div class="feature">
                    <div class="feature-icon">üñ•Ô∏è</div>
                    <h3 class="feature-title">User-Friendly GUI</h3>
                    <p class="feature-description">Step-by-step interface guides you through the analysis. No command-line experience required‚Äîjust click and process.</p>
                </div>
            </div>

            <section class="section">
                <h2 class="section-title">Processing Pipeline Overview</h2>
                <p class="section-content">
                    MPS implements a sophisticated multi-step pipeline based on the latest calcium imaging analysis methods:
                </p>
                
                <div class="code-block">
Raw Videos ‚Üí Deglow + Denoise ‚Üí Motion Correction ‚Üí 
NNDSVD Initialization ‚Üí Watershed Segmentation ‚Üí Spatial Merging ‚Üí 
Temporal Extraction ‚Üí Spatial Noise Estimation ‚Üí Temporal Update (CNMF) ‚Üí 
Spatial Update (CNMF) ‚Üí Temporal Update (CNMF) ‚Üí Quality Control ‚Üí Export
                </div>
                
                <p class="section-content">
                    Each step is optimized for miniscope data, with parameters that can be fine-tuned for your specific experimental conditions. The GUI provides real-time feedback and visualization at every stage.
                </p>
            </section>
        </div>

        <!-- Documentation Tab -->
        <div id="documentation" class="tab-content">
            <section class="section">
                <h2 class="section-title">üìö Miniscope Processing Pipeline Guide</h2>
                
                <div class="doc-container">
                    <h3>Overview</h3>
                    <p>This guide walks through the complete miniscope calcium imaging processing pipeline.</p>
                    
                    <h3>Quick Start</h3>
                    <ol>
                        <li><strong>Initial Setup</strong>
                            <ul>
                                <li>Install MPS using the Windows installer</li>
                                <li>Double-click the MPS desktop icon to launch</li>
                            </ul>
                        </li>
                        <li><strong>Loading Data</strong>
                            <ul>
                                <li><strong>New Analysis</strong>: Start with Step 1: Project Configuration</li>
                                <li><strong>Existing Analysis</strong>:
                                    <ul>
                                        <li>File ‚Üí Load parameters file (if you have saved parameters)</li>
                                        <li>Data ‚Üí Load previous data (to continue from a checkpoint)</li>
                                        <li>Enable automation ‚Üí Automation ‚Üí Toggle automation</li>
                                        <li>Run automation ‚Üí Run all steps or Run from current step</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ol>
                </div>

                <div class="doc-toc">
                    <h3>Table of Contents</h3>
                    <ul>
                        <li><a href="#getting-started">Getting Started</a></li>
                        <li><a href="#pipeline-steps">Pipeline Steps</a>
                            <ul style="margin-left: 20px;">
                                <li><a href="#step-1">Step 1: Project Configuration</a></li>
                                <li><a href="#step-2">Step 2: Data Preprocessing</a></li>
                                <li><a href="#step-3">Step 3: Spatial Cropping and Initialization</a></li>
                                <li><a href="#step-4">Step 4: Component Detection</a></li>
                                <li><a href="#step-5">Step 5: CNMF Preparation</a></li>
                                <li><a href="#step-6">Step 6: CNMF Processing</a></li>
                                <li><a href="#step-7">Step 7: Spatial Refinement</a></li>
                                <li><a href="#step-8">Step 8: Final Processing and Export</a></li>
                            </ul>
                        </li>
                        <li><a href="#tips">Tips and Best Practices</a></li>
                        <li><a href="#issues">Common Issues and Solutions</a></li>
                        <li><a href="#interpreting">Interpreting Your Results</a></li>
                        <li><a href="#advanced">Advanced Features</a></li>
                    </ul>
                </div>

                <div id="getting-started" class="doc-section">
                    <h3>Getting Started</h3>
                    
                    <h4>Step A: Initial Setup</h4>
                    <p>Launch MPS by double-clicking the desktop icon created during installation</p>
                    <p>The application will open ready for use - no coding required!</p>
                    
                    <h4>Step B: Loading Data</h4>
                    <ul>
                        <li><strong>Existing parameter file</strong> ‚Üí File ‚Üí Load parameters file</li>
                        <li><strong>Enable automation</strong> ‚Üí Automation ‚Üí Toggle automation</li>
                        <li><strong>Load previous data</strong> ‚Üí Load previous data (click the directory containing the cache_path) ‚Üí mark through completed n-1 step you want to land on</li>
                        <li><strong>Run automation</strong> ‚Üí Run all steps or Run from current step depending on where you are</li>
                    </ul>
                </div>

                <div id="pipeline-steps" class="doc-section">
                    <h3>Pipeline Steps</h3>
                    
                    <div id="step-1" class="parameter-box">
                        <h4>Step 1: Project Configuration</h4>
                        <p><strong>Required inputs:</strong></p>
                        <ul>
                            <li>Animal ID</li>
                            <li>Session ID</li>
                            <li>Input directory (directory where videos are)</li>
                            <li>Output Directory (Where you want the results to go)</li>
                        </ul>
                        <p><strong>Advanced Settings:</strong> Configure dask based on your machine setup. Default settings:</p>
                        <ul>
                            <li>8 workers</li>
                            <li>200 GB memory limit</li>
                            <li>100% video percentage (decrease for testing)</li>
                        </ul>
                    </div>

                    <div id="step-2" class="parameter-box">
                        <h4>Step 2: Data Preprocessing</h4>
                        
                        <h5>Step 2a: File Pattern Recognition</h5>
                        <ul>
                            <li><strong>File pattern</strong>: Use regex to match your video files. For files ending in .avi, the pattern is already set</li>
                            <li><strong>Tip</strong>: Plug a video path example into an LLM and it'll generate the regex you need</li>
                        </ul>
                        <p><strong>Options:</strong></p>
                        <ul>
                            <li>Don't worry about downsampling if your computer has sufficient resources</li>
                            <li>Line splitting detection is optional (see <a href="#advanced">Advanced Features</a>)</li>
                        </ul>

                        <h5>Step 2b: Background Removal and Denoising</h5>
                        <p>This step cleans up your raw video data.</p>
                        <p><strong>Denoising Methods:</strong></p>
                        <ul>
                            <li><strong>Median</strong>: Takes the middle value in a neighborhood - best for salt-and-pepper noise, preserves edges well, general purpose winner</li>
                            <li><strong>Gaussian</strong>: Blurs everything smoothly - good for general noise but can lose sharp details, use when you need everything smooth</li>
                            <li><strong>Bilateral</strong>: Smart blur that preserves edges - slower but keeps neuron boundaries sharp while smoothing inside</li>
                            <li><strong>Anisotropic</strong>: Directional smoothing - follows the shape of structures, great for elongated neurons but slowest option</li>
                        </ul>
                        <p><strong>Background Removal Methods:</strong></p>
                        <ul>
                            <li><strong>Tophat</strong>: Classic approach - subtracts a morphologically opened version (think "blurred background"), works great for most data</li>
                            <li><strong>Uniform</strong>: Simple rolling average subtraction - faster, works when background changes are smooth and gradual</li>
                        </ul>

                        <h5>Step 2c: Motion Correction</h5>
                        <ul>
                            <li>Keep motion estimation on "frame" (default)</li>
                            <li>This is the longest step - expect 3x your video size in RAM usage</li>
                            <li>The algorithm uses recursive estimation with phase correlation</li>
                        </ul>

                        <h5>Step 2d: Quality Control</h5>
                        <ul>
                            <li><strong>Threshold factor</strong>: How many standard deviations the motion needs to be above the mean before frames get dropped</li>
                            <li>Identifies frames with excessive motion that might corrupt analysis</li>
                        </ul>

                        <h5>Step 2e: Data Validation</h5>
                        <ul>
                            <li>Keep fill value as zero (for NaNs)</li>
                            <li>Keep other options checked</li>
                            <li>Validates transformed data integrity</li>
                        </ul>

                        <h5>Step 2f: Preview Results</h5>
                        <ul>
                            <li>Subset of data validation</li>
                            <li>Nothing saved except statistics</li>
                            <li>Good gut check to ensure Step 2 went okay</li>
                        </ul>
                    </div>

                    <div id="step-3" class="parameter-box">
                        <h4>Step 3: Spatial Cropping and Initialization</h4>
                        
                        <h5>Step 3a: Define ROI</h5>
                        <ul>
                            <li><strong>Critical for performance</strong>: Get the crop sizing as small as possible</li>
                            <li><strong>Tip</strong>: Test on 10% of video or less, then come back with full video</li>
                            <li>Use circular crop centered on your imaging field</li>
                            <li>Adjust offset if field is not centered</li>
                        </ul>

                        <h5>Step 3b: NNDSVD Initialization</h5>
                        <p>Think of NNDSVD like a conductor listening to a recording of an orchestra warming up. Instead of trying to pick out each instrument one by one, the conductor listens to the whole cacophony and instantly recognizes "that's the violin section over there, the brass in the back, the woodwinds on the right." NNDSVD does the same - it listens to all your neural "instruments" playing at once and quickly identifies the major sections before fine-tuning each individual player.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Number of components</strong>: Increases data granularity with diminishing returns. Component zero is background noise</li>
                            <li><strong>Power iterations</strong>: Refines the singular value decomposition (5 is usually plenty)</li>
                            <li><strong>Sparsity threshold</strong>: Controls signal vs noise pickiness (0.05 = keep things 5% above noise floor)</li>
                            <li><strong>Spatial regularization</strong>: Helps components be "blob-like" instead of scattered pixels</li>
                            <li><strong>Chunk size</strong>: Speeds processing - decrease for less memory but slower processing</li>
                        </ul>

                        <h5>Step 3c: Early Analysis Option</h5>
                        <p>Some miniscope and neural signal papers stop here if you want preliminary results.</p>
                    </div>

                    <div id="step-4" class="parameter-box">
                        <h4>Step 4: Component Detection</h4>
                        
                        <h5>Step 4a: Watershed Parameter Search</h5>
                        <p>Watershed segmentation figures out how many neurons there are. Since it's better to overestimate, this quickly finds optimal parameters.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Min distances</strong>: Minimum pixel distance between neuron centers (10, 20, 30 tries different spacings)</li>
                            <li><strong>Threshold relativity</strong>: How much brighter than surroundings a peak needs to be (0.1 = 10% brighter)</li>
                            <li><strong>Sigma values</strong>: Smoothing before finding peaks - like trying different glasses prescriptions (1.0 = sharp, 2.0 = slightly blurry)</li>
                            <li><strong>Sample size</strong>: How many components to test (20 is usually enough)</li>
                        </ul>

                        <h5>Step 4b: Apply Best Parameters</h5>
                        <ul>
                            <li>Uses the best parameters from 4a</li>
                            <li><strong>Minimum region size</strong>: Smallest neuron size you'll accept (better to overestimate, so really small is okay)</li>
                        </ul>

                        <h5>Step 4c: Merging Units</h5>
                        <p>Collapses spatially overlapping components that were oversegmented in 4b.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Distance Threshold</strong>: How close (in pixels) two neuron centers need to be to consider merging (25 pixels is a good start)</li>
                            <li><strong>Size Ratio Threshold</strong>: Won't merge neurons if one is way bigger than the other (5.0 = one can be up to 5x bigger)</li>
                            <li><strong>Minimum Component Size</strong>: Tosses out anything smaller than this (9 pixels = 3x3 square minimum)</li>
                        </ul>
                        <p>The whole point: we overshot in 4b and now we're cleaning up by merging things that are probably the same neuron.</p>

                        <h5>Step 4d: Temporal Signal Extraction</h5>
                        <p>Extracts the actual calcium traces from each spatial component - going from "here's where neurons are" to "here's what they're doing over time."</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Batch Size</strong>: Components to process at once (smaller = less memory but slower, 10 is safe)</li>
                            <li><strong>Frame Chunk Size</strong>: Frames to load at once - adjust based on RAM (10000 is good for most systems)</li>
                            <li><strong>Component Limit</strong>: For testing - process just a subset before doing the full run</li>
                            <li><strong>Memory Management</strong>: Keep these on unless you have unlimited RAM and want to live dangerously</li>
                        </ul>

                        <h5>Step 4e: AC Initialization</h5>
                        <p>Prepares the spatial (A) and temporal (C) matrices for the final CNMF algorithm.</p>
                        <p><strong>Spatial Normalization options:</strong></p>
                        <ul>
                            <li><strong>max</strong>: Normalize each component to its brightest pixel (default, works well)</li>
                            <li><strong>l1</strong>: Normalize by total brightness - use when component sizes vary a lot</li>
                            <li><strong>l2</strong>: Normalize by "energy" - mathematical but can be useful</li>
                            <li><strong>none</strong>: Raw values - only if you know what you're doing</li>
                        </ul>
                        <p><strong>Skip Background</strong>: Usually yes - component 0 is background from NNDSVD</p>

                        <h5>Step 4f: Final Component Preparation</h5>
                        <p>Quality control - removes obviously broken components before expensive processing.</p>
                        <ul>
                            <li><strong>Remove NaN Components</strong>: Always remove these, they'll break everything</li>
                            <li><strong>Remove Empty Components</strong>: Ghost neurons that don't exist</li>
                            <li><strong>Remove Flat Components</strong>: Dead pixels or artifacts</li>
                            <li><strong>Maximum Components</strong>: Limit for testing or if you only care about the strongest signals</li>
                        </ul>

                        <h5>Step 4g: Temporal Merging</h5>
                        <p>Final cleanup - merges components that are probably the same neuron but got split up.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Temporal Correlation Threshold</strong>: How similar calcium traces need to be (0.75 = 75% similar, higher = more conservative)</li>
                            <li><strong>Spatial Overlap Threshold</strong>: How much components need to overlap spatially (0.3 = 30% overlap minimum)</li>
                        </ul>
                        <p>Why this matters: Sometimes one neuron gets detected as 2-3 components, especially if it has complex shape or initial detection was too aggressive. This fixes that by looking for components that fire together (high correlation) and are in the same place (spatial overlap).</p>
                    </div>

                    <div id="step-5" class="parameter-box">
                        <h4>Step 5: CNMF Preparation</h4>
                        
                        <h5>Step 5a: Noise Estimation</h5>
                        <p>Figures out how noisy each pixel is across your field of view. Critical for CNMF because it tells the algorithm which parts of the signal to trust more.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Noise Scaling Factor</strong>: In active regions (where neurons are), noise is usually higher due to shot noise from calcium indicators. This factor (1.5 default) scales up the noise estimate in bright areas</li>
                            <li><strong>Smoothing Sigma</strong>: Smooths the noise map spatially - helps avoid weird pixel-to-pixel variations (1.0 = gentle smoothing)</li>
                            <li><strong>Background Threshold</strong>: How to decide what's "active" vs "background"
                                <ul>
                                    <li><strong>mean</strong>: Use average brightness as cutoff (good default)</li>
                                    <li><strong>median</strong>: Use middle value - more robust if you have outliers</li>
                                    <li><strong>custom</strong>: Set your own threshold</li>
                                </ul>
                            </li>
                        </ul>

                        <h5>Step 5b: Validation and Setup</h5>
                        <p>Quality control checkpoint - validates data and optionally filters components by size before expensive CNMF computation.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Check for NaN/Inf</strong>: These will break CNMF, so always check (can be slow on huge datasets)</li>
                            <li><strong>Compute Full Statistics</strong>: Get detailed stats on components - useful for troubleshooting</li>
                            <li><strong>Size Filtering</strong>: Remove components obviously too small or large to be neurons
                                <ul>
                                    <li>Minimum size: 10 pixels is reasonable</li>
                                    <li>Maximum size: 1000 pixels - anything larger might be merged neurons or artifacts</li>
                                </ul>
                            </li>
                        </ul>
                    </div>

                    <div id="step-6" class="parameter-box">
                        <h4>Step 6: CNMF Processing</h4>
                        
                        <h5>Step 6a: YrA Computation</h5>
                        <p>Computes YrA (Y residual times A) - figuring out how much of each pixel's activity can be explained by each component. This is the computational bottleneck of CNMF.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Component Source</strong>: Always uses filtered components from 5b now</li>
                            <li><strong>Subtract Background</strong>: Remove background signal before computing YrA (recommended)</li>
                            <li><strong>Use Float32</strong>: Cut memory usage in half with minimal precision loss (always recommended)</li>
                            <li><strong>Fix NaN Values</strong>: Replace any NaN values with zeros before computation</li>
                        </ul>

                        <h5>Step 6b: YrA Validation</h5>
                        <p>Sanity check on YrA computation - ensures nothing went wrong and gives quality metrics.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Number of Units to Analyze</strong>: Sample size for validation (5 is usually enough to spot issues)</li>
                            <li><strong>Frame Selection Method</strong>:
                                <ul>
                                    <li><strong>random</strong>: Random chunk of frames</li>
                                    <li><strong>start/middle/end</strong>: Specific sections of the recording</li>
                                    <li><strong>highest_variance</strong>: Would find the most active period (not implemented yet)</li>
                                </ul>
                            </li>
                            <li><strong>Number of Frames</strong>: How many frames to analyze (1000 is plenty for validation)</li>
                            <li><strong>Correlation Analysis</strong>: Check if units are correlated (they shouldn't be after all our processing)</li>
                            <li><strong>Detailed Statistics</strong>: Compute skewness, kurtosis, temporal stability - fancy stats that can reveal issues</li>
                        </ul>
                        <p><strong>What YrA actually means</strong>: YrA is essentially asking "for each neuron, how well does its spatial footprint explain the activity at each time point?" High YrA values mean that neuron's shape matches the activity pattern well at that time. This is used by CNMF to refine both the spatial footprints and temporal traces iteratively.</p>

                        <h5>Step 6c: Parameter Suggestion for Temporal Update</h5>
                        <p>Analyzes your data to suggest optimal parameters for CNMF temporal update</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Components to Analyze</strong>: How many components to sample for analysis (20 is usually enough)</li>
                            <li><strong>Frames to Analyze</strong>: How many frames to look at (5000 gives a good sample)</li>
                            <li><strong>Component Selection</strong>:
                                <ul>
                                    <li><strong>random</strong>: Random sampling - good default</li>
                                    <li><strong>best_snr</strong>: Analyze the cleanest components</li>
                                    <li><strong>worst_snr</strong>: Analyze the noisiest components (useful for troubleshooting)</li>
                                    <li><strong>largest/smallest</strong>: Based on spatial footprint size</li>
                                </ul>
                            </li>
                        </ul>
                        <p>The analysis examines:</p>
                        <ul>
                            <li>Signal-to-noise ratio (SNR) distribution</li>
                            <li>Temporal dynamics (how "bursty" vs smooth the signals are)</li>
                            <li>AR coefficient strengths (how much temporal correlation exists)</li>
                            <li>Spike rates (how active the neurons are)</li>
                        </ul>
                        <p>Based on this, it suggests:</p>
                        <ul>
                            <li><strong>AR order (p)</strong>: 1 for smooth traces, 2 for more complex dynamics</li>
                            <li><strong>Sparse penalty</strong>: Controls sparsity of spike inference (lower = more spikes)</li>
                            <li><strong>Max iterations</strong>: How long to run the optimizer</li>
                            <li><strong>Zero threshold</strong>: When to consider a spike "real"</li>
                        </ul>

                        <h5>Step 6d: Update Temporal Components</h5>
                        <p>The heavy lifting step - runs the actual CNMF temporal update using the suggested parameters.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>AR Order</strong>: Order of the autoregressive model (1-2 typical)</li>
                            <li><strong>Sparse Penalty</strong>: L1 penalty for spike sparsity (smaller = more spikes detected)</li>
                            <li><strong>Max Iterations</strong>: Solver iterations (500 usually enough)</li>
                            <li><strong>Zero Threshold</strong>: Values below this are set to zero</li>
                            <li><strong>Normalize</strong>: Whether to normalize traces (usually yes)</li>
                            <li><strong>Chunk Size</strong>: How many frames to process at once (5000 is good)</li>
                            <li><strong>Overlap</strong>: Frames shared between chunks to avoid edge artifacts (100 works well)</li>
                            <li><strong>Dask Settings</strong> (for parallel processing):
                                <ul>
                                    <li><strong>Workers</strong>: How many parallel processes (8 is good for most systems)</li>
                                    <li><strong>Memory per Worker</strong>: RAM limit per process (adjust based on your system)</li>
                                    <li><strong>Threads per Worker</strong>: CPU threads per process (match your CPU cores)</li>
                                </ul>
                            </li>
                        </ul>
                        <p>This step outputs:</p>
                        <ul>
                            <li><strong>C</strong>: Denoised calcium traces</li>
                            <li><strong>S</strong>: Inferred spike trains</li>
                            <li><strong>b0/c0</strong>: Background components</li>
                            <li><strong>g</strong>: AR coefficients</li>
                        </ul>

                        <h5>Step 6e: Filter and Validate</h5>
                        <p>Quality control after temporal update - removes components that didn't optimize well.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Min Spike Sum</strong>: Components with almost no spikes are probably noise (1e-6 catches dead components)</li>
                            <li><strong>Min Calcium Variance</strong>: Flat traces indicate dead pixels or artifacts (1e-6 is reasonable)</li>
                            <li><strong>Min Spatial Sum</strong>: Components need some spatial extent (1e-6 removes empty components)</li>
                        </ul>
                        <p>Components that pass all filters are your "good" neurons.</p>
                    </div>

                    <div id="step-7" class="parameter-box">
                        <h4>Step 7: Spatial Refinement</h4>
                        
                        <h5>Step 7a: Spatial Component Dilation</h5>
                        <p>Expands the spatial footprints for visualization and ROI analysis. The CNMF footprints are often conservative (tight around the neuron), so we dilate them for better coverage.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Dilation Window Size</strong>: Radius of the structuring element (3 pixels is typical)
                                <ul>
                                    <li>Larger = more expansion, risk of merging nearby neurons</li>
                                    <li>Smaller = conservative expansion</li>
                                </ul>
                            </li>
                            <li><strong>Intensity Threshold</strong>: Only dilate pixels above this fraction of component max (0.1 = 10%)
                                <ul>
                                    <li>Prevents dilating into noise</li>
                                    <li>Higher threshold = more conservative dilation</li>
                                </ul>
                            </li>
                        </ul>
                        <p>The dilated components are used for:</p>
                        <ul>
                            <li>Creating ROI masks for further analysis</li>
                            <li>Visualization (easier to see boundaries)</li>
                            <li>Neuropil estimation (defining regions around neurons)</li>
                        </ul>

                        <h5>Step 7b: Component Clustering</h5>
                        <p>Groups nearby components into clusters for efficient spatial refinement processing.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Max Cluster Size</strong>: Maximum components per cluster (10 default)</li>
                            <li><strong>Min Area</strong>: Minimum area to consider a component valid (20 pixels default)</li>
                            <li><strong>Min Intensity</strong>: Minimum intensity threshold (0.1 default)</li>
                            <li><strong>Overlap Threshold</strong>: How much components can overlap before clustering (0.2 = 20% default)</li>
                        </ul>
                        <p>This step enables processing components in local regions rather than the entire field of view, dramatically improving efficiency.</p>

                        <h5>Step 7c: Component Boundary Calculation</h5>
                        <p>Creates "bounding boxes" around clustered neurons - like drawing rectangles around groups of neurons that are close together.</p>
                        <p><strong>Key Parameters:</strong></p>
                        <ul>
                            <li><strong>Dilation Radius</strong> (10 pixels default): How much to expand neuron footprints before calculating bounds
                                <ul>
                                    <li>Larger radius = bigger bounding boxes, more conservative</li>
                                    <li>Smaller radius = tighter boxes, may miss parts of neurons</li>
                                </ul>
                            </li>
                            <li><strong>Padding</strong> (20 pixels default): Extra space added around the dilated shapes
                                <ul>
                                    <li>Ensures you capture the full extent of neural activity</li>
                                    <li>Important for neurons near edges of clusters</li>
                                </ul>
                            </li>
                            <li><strong>Minimum Size</strong> (40 pixels default): Smallest allowed bounding box dimension
                                <ul>
                                    <li>Prevents tiny boxes that can't contain meaningful neural activity</li>
                                </ul>
                            </li>
                            <li><strong>Intensity Threshold</strong> (0.05 default): What fraction of the component's maximum counts as "part of the neuron"
                                <ul>
                                    <li>Lower = more inclusive, larger bounds</li>
                                    <li>Higher = more restrictive, tighter bounds</li>
                                </ul>
                            </li>
                        </ul>
                        <p>The output is a set of rectangular regions, one for each cluster of neurons. These bounds are used in the next steps to:</p>
                        <ul>
                            <li>Limit spatial updates to relevant areas (faster processing)</li>
                            <li>Define regions for background estimation</li>
                            <li>Create local coordinate systems for optimization</li>
                        </ul>

                        <h5>Step 7d: Parameter Suggestions for Spatial Update</h5>
                        <p>Analyzes your data and suggests optimal parameters.</p>
                        <p><strong>Analysis Parameters:</strong></p>
                        <ul>
                            <li><strong>Number of Frames</strong> (1000 default): How many frames to analyze for statistics
                                <ul>
                                    <li>More frames = better statistics but slower</li>
                                    <li>1000 is usually sufficient</li>
                                </ul>
                            </li>
                            <li><strong>Sample Size</strong> (100 default): How many components to analyze (0 = all)
                                <ul>
                                    <li>Analyzing all components can be slow for large datasets</li>
                                    <li>100 gives a good representative sample</li>
                                </ul>
                            </li>
                        </ul>
                        <p><strong>What it analyzes:</strong></p>
                        <ul>
                            <li><strong>Temporal variability</strong>: How much each pixel's intensity changes over time</li>
                            <li><strong>Spatial coherence</strong>: How "neuron-like" the components look</li>
                            <li><strong>Signal strength</strong>: Distinguishing real neural signals from noise</li>
                            <li><strong>Component characteristics</strong>: Size, shape, compactness, circularity</li>
                        </ul>
                        <p><strong>Output recommendations:</strong></p>
                        <ul>
                            <li><strong>Minimum STD Threshold</strong>: Pixels below this variability are likely not neural signals
                                <ul>
                                    <li><strong>Conservative</strong>: Higher threshold, fewer false positives</li>
                                    <li><strong>Balanced</strong>: Good trade-off</li>
                                    <li><strong>Aggressive</strong>: Lower threshold, more detections but possibly more noise</li>
                                </ul>
                            </li>
                            <li><strong>Penalty Scale</strong>: Controls sparsity in the spatial update
                                <ul>
                                    <li>Lower values = sparser solutions (fewer pixels per neuron)</li>
                                    <li>Higher values = denser solutions (more pixels per neuron)</li>
                                </ul>
                            </li>
                            <li><strong>Maximum Penalty</strong>: Upper bound for the penalty parameter
                                <ul>
                                    <li>Prevents the algorithm from being too restrictive</li>
                                </ul>
                            </li>
                        </ul>
                        <p><strong>The visualizations show:</strong></p>
                        <ul>
                            <li><strong>STD Distribution</strong>: How variable different pixels are</li>
                            <li><strong>Spatial Coherence</strong>: How well-formed the components are</li>
                            <li><strong>STD vs Size</strong>: Relationship between component size and signal strength</li>
                            <li><strong>Compactness vs Circularity</strong>: Shape characteristics of components</li>
                        </ul>
                        <p><strong>Common scenarios:</strong></p>
                        <ul>
                            <li>If your neurons look fragmented: Use lower penalty values</li>
                            <li>If neurons are merging: Use higher penalty values</li>
                            <li>If missing dim neurons: Use aggressive thresholds</li>
                            <li>If too much noise: Use conservative thresholds</li>
                        </ul>

                        <h5>Step 7e: Spatial Update</h5>
                        <p>Updates spatial components using multi-penalty LASSO regression on local video regions defined by the bounds calculated in Step 7c.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Number of Frames</strong>: Number of frames to use for spatial update</li>
                            <li><strong>Min Penalty</strong>: Minimum LASSO penalty value</li>
                            <li><strong>Max Penalty</strong>: Maximum LASSO penalty value</li>
                            <li><strong>Num Penalties</strong>: Number of penalty values to try</li>
                            <li><strong>Min STD</strong>: Minimum pixel STD to consider for update</li>
                            <li><strong>Progress Interval</strong>: How often to report progress (pixels)</li>
                            <li><strong>Show incremental updates</strong>: Display component updates as they're processed</li>
                        </ul>
                        <p>This step ensures spatial footprints accurately represent the neurons after all the temporal processing.</p>

                        <h5>Step 7f: Merging and Validation</h5>
                        <p>Final spatial processing step - merges the updated spatial components from Step 7e, handles component overlaps, and validates the final results.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Apply smoothing</strong>: Whether to apply Gaussian smoothing to merged components</li>
                            <li><strong>Smoothing Sigma</strong>: Gaussian filter sigma for smoothing</li>
                            <li><strong>Handle overlaps</strong>: Whether to normalize overlapping components</li>
                            <li><strong>Min Component Size</strong>: Minimum size of components to keep (pixels)</li>
                            <li><strong>Save both versions</strong>: Save both raw and smoothed versions</li>
                        </ul>
                        <p>This produces the final spatial components ready for use in subsequent analysis.</p>
                    </div>

                    <div id="step-8" class="parameter-box">
                        <h4>Step 8: Final Processing and Export</h4>
                        <p>The final stage of the pipeline consists of three critical steps that prepare your data for analysis.</p>
                        
                        <h5>Step 8a: YrA Computation</h5>
                        <p>This step computes the residual activity (YrA) using the updated spatial components from Step 7e/7f. YrA represents the "leftover" signal after accounting for all other components - it's what each neuron sees after removing the contributions of all other neurons.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>Spatial Component Source</strong>: Which spatial components to use (step7f_A_merged is preferred)</li>
                            <li><strong>Temporal Component Source</strong>: Which temporal components to use for the calculation</li>
                            <li><strong>Subtract Background</strong>: Remove background contribution (recommended)</li>
                            <li><strong>Use Float32</strong>: Reduces memory usage by half with minimal precision loss</li>
                            <li><strong>Fix NaN Values</strong>: Replace any NaN values with zeros</li>
                        </ul>
                        <p><strong>What this does</strong>: YrA is essentially asking "what signal remains at each pixel after we subtract out all the other neurons' contributions?" This residual is then projected onto each neuron's spatial footprint to get its "pure" temporal trace.</p>

                        <h5>Step 8b: Final Temporal Update</h5>
                        <p>Updates temporal components (C) and spike estimates (S) using the YrA computed in Step 8a, with CVXPY optimization using AR modeling and sparsity constraints.</p>
                        <p><strong>Parameters:</strong></p>
                        <ul>
                            <li><strong>AR Order (p)</strong>: Order of autoregressive model (2 for complex dynamics, 1 for smoother)</li>
                            <li><strong>Sparse Penalty</strong>: L1 penalty for spike sparsity (lower = more spikes detected)</li>
                            <li><strong>Max Iterations</strong>: Maximum solver iterations (500 usually sufficient)</li>
                            <li><strong>Zero Threshold</strong>: Values below this are set to zero</li>
                            <li><strong>Normalize</strong>: Whether to normalize traces (usually yes)</li>
                            <li><strong>Include background</strong>: Incorporate background components (b, f)</li>
                            <li><strong>Chunk Size</strong>: Frames per temporal chunk for parallel processing</li>
                            <li><strong>Chunk Overlap</strong>: Overlap between chunks to avoid edge artifacts</li>
                        </ul>
                        <p><strong>Processing approach</strong>: The algorithm uses temporal chunking for better memory efficiency and parallel processing. Each chunk is processed independently then merged, like editing a movie in segments then stitching them together.</p>
                        <p>This ensures consistency between spatial and temporal representations while maintaining biologically plausible calcium dynamics.</p>

                        <h5>Step 8c: Final Filtering and Data Export</h5>
                        <p>The culmination of the entire pipeline - this step performs final quality filtering, exports the data in multiple formats, and generates summary statistics and validation plots.</p>
                        <p><strong>Filtering Criteria:</strong></p>
                        <ul>
                            <li><strong>Min Component Size</strong>: Minimum size in pixels (components smaller than this are removed)</li>
                            <li><strong>Min Signal-to-Noise Ratio</strong>: Minimum SNR threshold</li>
                            <li><strong>Min Correlation</strong>: Minimum correlation coefficient</li>
                        </ul>
                        <p><strong>Export Options:</strong></p>
                        <ul>
                            <li><strong>Zarr format</strong>: Efficient for large-scale analysis with chunked storage</li>
                            <li><strong>NumPy format</strong>: Compatible with most Python analysis pipelines</li>
                            <li><strong>JSON format</strong>: Human-readable metadata and component IDs</li>
                            <li><strong>Pickle format</strong>: Complete Python objects for easy loading</li>
                        </ul>
                        <p><strong>Summary Plots:</strong></p>
                        <ul>
                            <li>Component spatial maps: Visual representation of where neurons are</li>
                            <li>Temporal traces: What neurons are doing over time</li>
                            <li>Quality metrics: Distribution of component sizes, amplitudes, and other statistics</li>
                        </ul>
                        <p><strong>Why multiple formats?</strong> Different downstream analyses prefer different formats:</p>
                        <ul>
                            <li>Zarr for big data analysis with out-of-memory computation</li>
                            <li>NumPy for traditional Python scientific computing</li>
                            <li>JSON for sharing results with non-Python tools</li>
                            <li>Pickle for quick Python-to-Python transfer with all metadata preserved</li>
                        </ul>
                        <p>The export includes a timestamp and all processing parameters, ensuring full reproducibility.</p>
                    </div>
                </div>

                <div id="tips" class="doc-section">
                    <h3>Tips and Best Practices</h3>
                    <ol>
                        <li><strong>Start small</strong>: Test parameters on 10% of your data before running the full pipeline</li>
                        <li><strong>Monitor memory</strong>: The pipeline is memory-intensive, especially Step 2c</li>
                        <li><strong>Save frequently</strong>: Enable autosave in the automation menu</li>
                        <li><strong>Check intermediate results</strong>: Use the visualization steps (2f, 6b) to verify processing quality</li>
                        <li><strong>Adjust for your data</strong>: Default parameters work for most data, but your specific recording might need tweaks</li>
                        <li><strong>When in doubt, oversegment</strong>: It's easier to merge components later than to split them</li>
                        <li><strong>Trust the parameter suggestions</strong>: Steps 4a and 7d analyze your specific data to recommend settings</li>
                        <li><strong>Use temporal chunking in Step 8</strong>: This dramatically reduces memory usage for long recordings</li>
                        <li><strong>Export multiple formats</strong>: Different downstream analyses may prefer different formats</li>
                        <li><strong>Document your parameters</strong>: The pipeline saves all parameters automatically for reproducibility</li>
                    </ol>
                </div>

                <div id="issues" class="doc-section">
                    <h3>Common Issues and Solutions</h3>
                    
                    <h4>Memory errors during YrA computation:</h4>
                    <ul>
                        <li>Reduce the number of components processed at once</li>
                        <li>Use float32 precision</li>
                        <li>Increase dask memory limits</li>
                    </ul>

                    <h4>Components look fragmented after spatial update:</h4>
                    <ul>
                        <li>Decrease penalty values in Step 7e</li>
                        <li>Check if minimum STD threshold is too high</li>
                    </ul>

                    <h4>Too many spurious components detected:</h4>
                    <ul>
                        <li>Increase minimum component size thresholds</li>
                        <li>Use conservative parameter suggestions</li>
                        <li>Increase distance threshold in watershed segmentation</li>
                    </ul>

                    <h4>Temporal traces look noisy:</h4>
                    <ul>
                        <li>Increase AR order (try p=2 instead of p=1)</li>
                        <li>Adjust sparse penalty (try higher values)</li>
                        <li>Check if noise estimation in Step 5a is accurate</li>
                    </ul>

                    <h4>Background components missing:</h4>
                    <ul>
                        <li>Ensure Step 3b includes component 0</li>
                        <li>Check that background removal in Step 2b isn't too aggressive</li>
                    </ul>

                    <h4>Step 8a YrA computation fails:</h4>
                    <ul>
                        <li>Check that spatial and temporal components have matching unit IDs</li>
                        <li>Ensure all required data from previous steps is available</li>
                        <li>Try reducing to a subset of components for testing</li>
                    </ul>

                    <h4>CVXPY solver failures in Step 8b:</h4>
                    <ul>
                        <li>Reduce max iterations if taking too long</li>
                        <li>Adjust solver tolerances (the code tries ECOS first, then SCS)</li>
                        <li>Check for components with all-zero traces</li>
                    </ul>

                    <h4>Memory errors during Step 8:</h4>
                    <ul>
                        <li>Use temporal chunking with smaller chunk sizes</li>
                        <li>Process components in batches</li>
                        <li>Close other applications to free RAM</li>
                        <li>Consider using a machine with more memory</li>
                    </ul>
                </div>

                <div id="interpreting" class="doc-section">
                    <h3>Interpreting Your Results</h3>
                    <p>After completing the pipeline, you'll have several key outputs:</p>
                    
                    <h4>Spatial Components (A)</h4>
                    <ul>
                        <li><strong>What they show</strong>: The spatial footprint of each neuron</li>
                        <li><strong>What to look for</strong>: Clear, contiguous regions roughly matching expected neuron size</li>
                        <li><strong>Red flags</strong>: Highly fragmented components, components much larger than expected neurons</li>
                    </ul>

                    <h4>Temporal Components (C)</h4>
                    <ul>
                        <li><strong>What they show</strong>: Denoised calcium traces for each neuron</li>
                        <li><strong>What to look for</strong>: Clear calcium transients with good signal-to-noise ratio</li>
                        <li><strong>Red flags</strong>: Flat traces, excessive noise, unrealistic dynamics</li>
                    </ul>

                    <h4>Spike Estimates (S)</h4>
                    <ul>
                        <li><strong>What they show</strong>: Inferred spike times and amplitudes</li>
                        <li><strong>What to look for</strong>: Sparse events corresponding to calcium transients</li>
                        <li><strong>Red flags</strong>: Continuous spiking, no detected events</li>
                    </ul>

                    <h4>Quality Metrics</h4>
                    <ul>
                        <li><strong>Component size distribution</strong>: Should match expected neuron sizes for your preparation</li>
                        <li><strong>Signal amplitude distribution</strong>: Should show clear separation from noise</li>
                        <li><strong>Temporal correlation</strong>: Neurons shouldn't be perfectly correlated unless they're truly synchronized</li>
                    </ul>

                    <h4>Using Your Results</h4>
                    <p>The exported data can be used for:</p>
                    <ul>
                        <li><strong>Population analysis</strong>: Study ensemble activity patterns</li>
                        <li><strong>Single-cell analysis</strong>: Track individual neuron responses</li>
                        <li><strong>Behavioral correlation</strong>: Link neural activity to behavior</li>
                        <li><strong>Network analysis</strong>: Study functional connectivity</li>
                        <li><strong>Longitudinal studies</strong>: Track the same neurons across sessions</li>
                    </ul>
                    
                    <p>Remember: The pipeline provides cleaned signals, but biological interpretation requires domain knowledge. When in doubt, consult the original videos to verify that detected components correspond to real neurons.</p>
                </div>

                <div id="advanced" class="doc-section">
                    <h3>Advanced Features</h3>
                    
                    <h4>Line Splitting Detection</h4>
                    <p>Some miniscope systems experience line splitting artifacts where signal appears in the leftmost pixels of frames. The pipeline can automatically detect and remove these frames.</p>
                    
                    <p><strong>Implementation:</strong></p>
                    <div class="code-block">import numpy as np
def detect_line_splitting_frames(xarray_data):
    """
    Detect line splitting frames by looking for signal in the leftmost 20 pixels.
    
    Args:
        xarray_data: xarray DataArray with dimensions ['frame', 'height', 'width'] 
        
    Returns:
        list: Frame indices to drop, e.g. [45, 123, 456]
    """
    
    # Extract the leftmost 20 pixels for all frames
    left_edge = xarray_data.isel(width=slice(0, 20))
    
    # Calculate mean intensity for each frame in the left edge region
    left_edge_means = left_edge.mean(dim=['height', 'width']).compute()
    
    # Calculate overall statistics to set threshold
    overall_mean = left_edge_means.mean().item()
    overall_std = left_edge_means.std().item()
    
    # Set threshold - frames with signal significantly above background
    # Using mean + 2*std as threshold for detecting anomalous signal
    threshold = overall_mean + 2 * overall_std
    
    # Find frames that exceed the threshold (have signal in left edge)
    problematic_frames = np.where(left_edge_means > threshold)[0]
    
    # Convert to regular Python list for JSON serialization
    frame_indices_to_drop = problematic_frames.tolist()
    
    return frame_indices_to_drop</div>
                    
                    <p><strong>When to use</strong>: Enable this in Step 2a if you notice vertical lines or artifacts on the left edge of your videos.</p>

                    <h4>Batch Processing with Parameters</h4>
                    <p>The pipeline supports saving and loading parameter files for batch processing:</p>
                    <ul>
                        <li><strong>Save parameters</strong>: After completing a successful run, save parameters via File ‚Üí Save parameters</li>
                        <li><strong>Load parameters</strong>: For new datasets, load saved parameters via File ‚Üí Load parameters</li>
                        <li><strong>Automation</strong>: Enable automation to run multiple steps without intervention</li>
                    </ul>

                    <h4>Custom Preprocessing Functions</h4>
                    <p>You can add custom preprocessing functions in Step 2a by modifying the post_process parameter in the video loading function.</p>

                    <h4>Non-rigid Motion Correction</h4>
                    <p>For datasets with significant non-rigid motion, enable mesh-based correction in Step 2c by specifying a mesh size (e.g., (5,5) for a 5x5 control point grid).</p>
                </div>

                <div class="doc-section">
                    <h3>Automation Features</h3>
                    
                    <h4>Autorun Mode</h4>
                    <ul>
                        <li><strong>Toggle Autorun</strong>: Automation ‚Üí Toggle Autorun</li>
                        <li><strong>Configure delays</strong>: Automation ‚Üí Configure Autorun</li>
                        <li><strong>Run all steps</strong>: Automation ‚Üí Run All Steps</li>
                        <li><strong>Run from current</strong>: Automation ‚Üí Run From Current Step</li>
                    </ul>

                    <h4>Parameter Files</h4>
                    <ul>
                        <li>Load predefined parameters to ensure consistency across analyses</li>
                        <li>Parameters are automatically applied to each step during autorun</li>
                        <li>Auto-save feature preserves parameters after each step</li>
                    </ul>
                </div>

                <div class="doc-section">
                    <h3>System Requirements</h3>
                    
                    <h4>Minimum Requirements</h4>
                    <ul>
                        <li><strong>RAM</strong>: 32GB (64GB+ recommended)</li>
                        <li><strong>CPU</strong>: 8+ cores recommended</li>
                        <li><strong>Storage</strong>: SSD with 2x video size free space</li>
                        <li><strong>GPU</strong>: Not required but can accelerate some operations</li>
                    </ul>

                    <h4>Recommended Setup</h4>
                    <ul>
                        <li><strong>RAM</strong>: 128GB+ for large datasets</li>
                        <li><strong>CPU</strong>: 16+ cores for parallel processing</li>
                        <li><strong>Storage</strong>: NVMe SSD for fastest I/O</li>
                        <li><strong>Network</strong>: Fast connection if using network storage</li>
                    </ul>
                </div>

                <div class="doc-section">
                    <h3>Final Notes</h3>
                    <p>This pipeline transforms raw calcium imaging videos into clean, separated signals from individual neurons. Each step builds on the previous ones, gradually refining the separation between signal and noise, between different neurons, and between neural activity and background.</p>
                    
                    <p>The key insight is that neural signals have both spatial structure (the shape of the neuron) and temporal structure (how the calcium concentration changes over time). By iteratively refining our estimates of both, we can achieve much better separation than by considering either alone.</p>
                    
                    <p>Remember: perfect is the enemy of good. The goal is biologically meaningful signals, not mathematical perfection. When in doubt, preserve more components rather than fewer - you can always exclude them in downstream analysis.</p>
                </div>

                <div class="doc-section">
                    <h3>Contributing and Support</h3>
                    <p>For issues, questions, or contributions:</p>
                    <ol>
                        <li>Check the troubleshooting section first</li>
                        <li>Review intermediate outputs to identify where problems occur</li>
                        <li>Save your parameter file and share it when reporting issues</li>
                        <li>Include system specifications (RAM, CPU, GPU) when reporting performance issues</li>
                    </ol>
                    
                    <p style="font-size: 20px; margin-top: 30px; text-align: center;">Happy processing!</p>
                </div>
            </section>
        </div>

        <!-- Installation Tab -->
        <div id="installation" class="tab-content">
            <div class="cta-section">
                <h2 class="cta-title">Ready to Process Your Data?</h2>
                <p class="cta-subtitle">Download MPS and start processing in minutes</p>
                <a href="https://github.com/ariasarch/MPS_installer/releases/latest/download/MPS_Setup_v1.0.0.exe" class="download-button">
                    <span>‚¨á</span>
                    <span>Download for Windows</span>
                </a>
                <p class="download-info">Windows 10/11 ‚Ä¢ 64-bit ‚Ä¢ ~10 GB disk space required</p>
                
                <div class="warning-box">
                    <strong>Note:</strong> Windows may show an "Unknown Publisher" warning. This is normal for software from individual developers and research labs. Click "More info" ‚Üí "Run anyway" to proceed with installation.
                </div>
            </div>

            <section class="section">
                <h2 class="section-title">Installation Process</h2>
                <p class="section-content">
                    The installer handles everything automatically:
                </p>
                <ol style="color: #cccccc; font-size: 18px; line-height: 2; margin-left: 20px;">
                    <li>Downloads MPS from GitHub</li>
                    <li>Creates desktop and Start Menu shortcuts</li>
                    <li>On first run, automatically sets up the Python environment (15-20 minutes)</li>
                    <li>After initial setup, MPS launches instantly</li>
                </ol>
            </section>

            <div class="requirements">
                <h3>System Requirements</h3>
                <ul>
                    <li>Windows 10 or 11 (64-bit)</li>
                    <li>16 GB RAM minimum (32 GB recommended for large datasets)</li>
                    <li>10 GB available disk space</li>
                    <li>Stable internet connection for installation</li>
                    <li>Git for Windows (installer will prompt if missing)</li>
                </ul>
            </div>
        </div>

        <!-- Resources Tab -->
        <div id="resources" class="tab-content">
            <section class="section">
                <h2 class="section-title">üîó Helpful Resources</h2>
                <div class="features">
                    <div class="feature">
                        <div class="feature-icon">üíª</div>
                        <h3 class="feature-title">Source Code</h3>
                        <p class="feature-description">Access the complete MPS source code, contribute improvements, or customize for your needs.</p>
                        <a href="https://github.com/ariasarch/MPS_1.0.0" class="footer-link" style="color: #4a9eff;">View on GitHub ‚Üí</a>
                    </div>
                    
                    <div class="feature">
                        <div class="feature-icon">üêû</div>
                        <h3 class="feature-title">Issue Tracker</h3>
                        <p class="feature-description">Report bugs, request features, or see what issues are being worked on.</p>
                        <a href="https://github.com/ariasarch/MPS_1.0.0/issues" class="footer-link" style="color: #4a9eff;">Report an Issue ‚Üí</a>
                    </div>
                    
                    <div class="feature">
                        <div class="feature-icon">üìß</div>
                        <h3 class="feature-title">Support</h3>
                        <p class="feature-description">Get help from the Neumaier Lab team for technical issues or analysis questions.</p>
                        <a href="mailto:support@neumaierlab.com" class="footer-link" style="color: #4a9eff;">Contact Support ‚Üí</a>
                    </div>
                </div>
            </section>
            
            <section class="section">
                <h2 class="section-title">Community</h2>
                <p class="section-content">
                    Join the growing community of researchers using MPS for their calcium imaging analysis. Share your experiences, learn from others, and contribute to making MPS even better.
                </p>
            </section>
        </div>
    </div>
    
    <footer>
        <div class="footer-content">
            <div class="footer-links">
                <a href="https://github.com/ariasarch/MPS_1.0.0" class="footer-link">GitHub Repository</a>
                <a href="https://github.com/ariasarch/MPS_1.0.0/issues" class="footer-link">Report Issues</a>
                <a href="https://github.com/ariasarch/MPS_1.0.0/blob/main/README.md" class="footer-link">Documentation</a>
                <a href="mailto:support@neumaierlab.com" class="footer-link">Contact Support</a>
            </div>
            <p class="footer-text">¬© 2024 Neumaier Lab ‚Ä¢ MPS - Miniscope Processing Software</p>
        </div>
    </footer>

    <script>
        function showTab(tabName) {
            // Hide all tab contents
            const tabContents = document.querySelectorAll('.tab-content');
            tabContents.forEach(tab => {
                tab.classList.remove('active');
            });
            
            // Remove active class from all tabs
            const navTabs = document.querySelectorAll('.nav-tab');
            navTabs.forEach(tab => {
                tab.classList.remove('active');
            });
            
            // Show the selected tab content
            document.getElementById(tabName).classList.add('active');
            
            // Add active class to the clicked tab
            event.target.classList.add('active');
        }
    </script>
</body>
</html>
